# 1. Что такое машинное обучение? Кто такой Data Scientist? Как машинное обучение и наука о данных связаны с искусственным интеллектом?
МО - наука (и искусство) программирования вычислительных машин таким образом, чтобы они могли учиться на основе данных.
Более общее определение:
МО - область знаний, которая изучает способы обучения вычислительных машин без явного программирования.
Определение для инженера:
Говорят, что компьютерная программа обучается на опыте Е в отношении некоторой задачи Т и некоторой меры производительности Р, если её производительность в задаче Т, измеряемая с помощью Р, улучшается с накоплением опыта Т.
Машинное обучение - это 
- математика + статистика + программирование
- алгоритмический подход к обработке (больших) данных
- слабый искусственный интеллект 
Машинное обучение не является:
- полноценным искусственным интеллектом
- осведомленным о предметной области
- панацеей от всех проблем человечества

Искусственный интеллект - это интеллект, демонстрируемый машинами, в отличии от естественного интеллекта, проявляемого животными, включая людей.
...

data scientist - hacking skills $\cap$ math & statistics knowledge $\cap$ substantive expertise
machine learning - hacking skills $\cap$ math & statics knowledge
traditional research - math & statistics knowledge $\cap$ substantive expertise
danger zone - hacking skills $\cap$ substantive expertise
# 2. Уровни развития искусственного интеллекта (слабый, сильный, ANI, AGI, ASI).
ANI - artifical narrow intelligence aka weak ai - narrow capability - present (искусственный узкий интеллект, он же слабый ИИ - узкие возможности - присутствуют)
AGI - artifical intelligence aka strong ai - general capability - future? (искусственный интеллект, он же сильный ИИ - общие возможности - будущее?)
ASI - artifical super intelligence aka strong ai - transcedent capability - possible? (искусственный сверхинтеллект, он же сильный ии - запредельные возможности - возможны?)


# 3. История развития ИИ, МО и глубокого обучения
1950 - 1980: Искусственный интеллект (ранний искусственный интеллект вызывает волнения). инжиниринг создания интеллектуальных машин и программ
1951 -  2010: Машинное обучение (МО начинает процветать). Способность к обучению без явного программирования.
2011 - н.в.: Глубокое обучение (Прорыв в глубоком обучении привел к буму искусственного интеллекта). Обучение на основе глубокой нейронной сети

# 4. В каких областях применяется машинное обучение? Приведите примеры решения прикладных задач с помощью МО.
Распознавание изображений (турникет ГУАП), автоматический переводчик, медицинская диагностика, торговля на фондовом рынке, предотвращение онлайн мошенничества, виртуальный ассистент, фильтр спама, машины с автопилотом, продуктовая рекомендация, предсказание трафика, распознавание речи и др.

# 5. Постановка задачи обучения на примерах.
X - множество объектов
Y - множество ответов (предсказаний, оценок, прогнозов)
$\phi(x), \phi: X \rightarrow Y$ - неизвестная зависимость (целевая функция)
Дано:
{$x_1, \ \ldots, \ x_l$} $\subset X$ - обучающая выборка
$y_i = \phi(x), i = 1, \ \ldots, \ l$ - известные ответы
Найти: 
- $g(x, \theta), g: X \times Θ \rightarrow Y$ - алгоритм, функция принятия решений или параметрическая модель, приближающая $\phi$ на всей выборке Х.
- $\theta \in Θ$ - вектор параметров модели, такой, что $g(x, \theta) \thickapprox \phi(x)$ 


# 6. Описание объектов и ответов. Типы задач машинного обучения.
__Объекты:__
$f_j: X \rightarrow D_j, j = 1, \ \ldots, \ n$ - признаки объектов
Типы скалярных объектов:
- $D_j$ = { 0, 1 } - бинарный признак $f_j$;
- $|D_j| < \infty$ - номинальный признак $f_j$;
- $|D_j| < \infty$, $D_j$ - упорядоченно-порядковый признак $f_j$;
- $D_j = \R$ - количественный признак $f_j$: интервал или число.
Вектор ($f_1(x), \ \ldots, \ \f_n(x)) - признаковое описание объекта x.
Матриц признаков: F = $||f_j(x_i)||_{l\times n}$ = ($f_1(x_1) \ \ldots \ f_n(x_1)$
.                                                                    (...
.                                                                    ($f_1(x_l) \ \ldots \ f_n(x_l)$
__Ответы:__
Задачи обучения с учителем:
Заданы "ответы учителя" $y_i = \phi(x_i)$ на обучающих $x_i$ 
Для классификации:
- Y = { -1, +1 } - бинарная классификация (2 класса);
- Y = { 1, ..., M } - классификация между М не пересекающимися классами;
- Y = { 0, 1 }^M - M классов, которые могут пересекаться
Для регрессии:
- Y = R or Y = R^M.

Ранжинирование:
- Y - конечное отсортированное множество

Задачи обучения без учителя 
- ответов нет, но требуется что-то сделать с самими объектами

__Типы задач МО__:
Статистическое обучение с учителем:
- обучение по прецедентам;
- восстановление зависимости по эмпирическим данным;
- прдсказательное моделирование;
- аппроксимация функций по заданным точкам
Два основных типа задач - классификация и регрессия

# 7. Обучение с учителем, предсказательные модели. Приведите не менее 4-х примеров описания прикладных задач.
**Обучение с учителем** предполагает наличие размеченных данных, где каждой входной характеристике соответствует целевая метка. Вот примеры прикладных задач для предсказательных моделей
Модель - параметрическое семейство функций A = {$g(x, \theta) | \theta \in Θ$},
где g: $x \times Θ \rightarrow Y$ - фиксированная функция, 
Θ - множество допустимых значений параметров $\theta$
Пример:
Линейная модель с векторным параметров $\theta = (\theta_1, \ \ldots , \ \theta_n) \in R^n$:
$g(x, \theta) = \sum_{j=1}^n \theta_j f_j(x)$ - для регрессии и ранжирования, Y = R;
$g(x, \theta) = sign \sum_{j=1}^n \theta_j f_j(x)$ - для классификации, Y = { -1; +1 }
__Примеры:__
1) Кредитный скоринг (классификация) - оценка вероятности того, что клиент не вернет кредит. Данные: возраст, доход, кредитная история, уровень задолженности. Целевая переменная: классы "надежный" или "ненадежный".
2) Прогнозирование спроса на продукцию (регрессия) - оценка объема продаж товара в следующем месяце. Данные: исторические продажи, сезонность, акции, тренды. Целевая переменная: числовое значение объема продаж.
3) Диагностика заболеваний (классификация) - определение наличия заболевания по медицинским данным. Данные: результаты анализов, симптомы, история болезни. Целевая переменная: наличие или отсутствие заболевания.
4) Распознавание рукописного текста (классификация) - преобразование изображений рукописных символов в текст. Данные: изображение букв или цифр. Целевая переменная: классы символов или цифр.



# 8. Алгоритм обучения. Сведение задачи обучения к задаче оптимизации.
Процесс обучения с учителем состоит из 2-х этапов:
- Обучение:
Алгоритм обучения $\mu: (X \times X)^l \in Θ$ по выборке $X^l = (x_j, y_j)_{i=1}^l$ строит функцию $g(x, \theta)$, оценивая (оптимизируя) параметры модели $\theta \in Θ$.
$[(f_1(x_1) \ \ldots \ f_n(x_1))$                                 ($y_1$)]                      ($\theta_1$)
...                                                         $\rightarrow^\phi$    ...           $\rightarrow^\mu$         ...       = 0
$[(f_1(x_l) \ldots f_n(x_l))$                                    ($y_l$)]                      ($\theta_n$) 
- Применение:
Функция $g(x, \theta)$ для новых объектов $x_j'$ выдает ответы $g(x_j', \theta)$

$(f_1(x_1') \ \ldots \ f_n(x_1'))$                  $g((x_1', \theta))$
...                                      $\rightarrow^g$       ...
$(f_1(x_k') \ldots f_n(x_k'))$                    ($g(x_k',\theta)$) 

__Замена задачи обучения на минимизацию:__
Метод минимизации эмпирического риска:
$\mu(X^l) = arg \ min_{g \in A} Q(g, X^l)$
Пример: метод наименьших квадратов, квадратичная ошибка.


# 9. Оценивание моделей. Эмпирический риск и функция потерь.
Функция потерь ℒ(g, x): для заданного объекта $x \in X$ вычисляет величину ошибки алгоритма (функции) g $\in$ A на этом объекте. Ошибка тем больше, чем сильнее $g(x, \theta)$ отклоняется от правильного ответа $\phi(x)$.
Функция потерь для задач классификации:
- ℒ(g, x) = $[g(x, \theta) \neq \phi(x)]$ - индикатор ошибки.
Функция потерь для задач регрессии:
- ℒ(g, x) = $|g(x, \theta) - \phi(x)|$ - абсолютное значение ошибки
- ℒ(g, x) = $(g(x, \theta) - \phi(x))^2$ - квадратичная ошибка
__Эмпирический риск:__
- Нельзя заранее достоверно узнать, на сколько хорошо алгоритм g покажет себя на практике ("риск"), поскольку неизвестной истинный закон распределения данных P(x, y).
- Оценить и улучшить работу алгоритма g можно на заранее известной ограниченной обучающей выборке (закон больших чисел).
- __Эмпирический риск__ - способ оценки качества работы алгоритма g на всей обучающей выборке $X^l$.
- Функционал эмпирического риска: $\frac{1}{l}\sum_{i=1}^l ℒ(g, x_i)$


# 10. Что такое переобучение (overfitting) и недообучение (underfitting)? Как их можно избежать?
__Переобучение__ - данных мало, параметров слишком много, модель сложная, избыточно гибкая. Ключевая проблема в МО.
Из-за чего? 
- избыточные параметры в модели $g(x, \theta)$ "расходуются" на чрезмерно тонкую подгонку под обучающую выборку;
- выбор g из А производится по неполной информации $X^l$ 
Как обнаружить?
- Эмпирически, путем разбиения выборки train и test (для test должны быть известны правильные признаки)
Избавиться нельзя, можно минимизировать:
- увеличить объем обучающих данных
- накладывать ограничения на $\theta$ (регуляризация)
- минимизировать одну из теоретических оценок
- выбирать модель по оценкам обобщающей способности
Регуляризация  - уменьшение значений параметров (сокращение весов)
__Недообучение__ - данных много, параметров недостаточно, модель простая, негибкая
Из-за чего?
- слишком простая модель
- недостаточное время обучения
- недостаточно информативные признаки
Методы борьбы:
- усложнение модели
- долгое обучение
- инженерия признаков
- удаление регуляризации или ее ослабление
# 11. Одномерная и многомерная линейная регрессия.
__Линейная регрессия__ - это метод МО, используемый для прогнозирования числовой целевой переменной на основе одной или нескольких независимых переменных.
__Одномерная__ - используется, когда есть одна независимая переменная $x$
Мат. модель: $y = \omega_0 + \omega_1 x$, где 
y - предсказанное значение;
x - независимая переменная;
$\omega_1$ - коэффициент наклона (угловой коэффициент)
$\omega_0$ - свободный член (сдвиг)
__Многомерная__ - есть несколько независимых переменных $x_1, \ \ldots, \ x_n$
Мат. модель: $y = \omega_0 + \omega_1 x + \ \ldots, \ + \omega_n x$, где 
y - предсказанное значение;
$x_1, \ \ldots, \ x_n$ - независимые переменные 
$\omega_1, \ \ldots, \ \omega_n$ - веса (коэффициенты)
$\omega_0$ - свободный член


# 12. Конструирование признаков.
Использование интуиции для создания новых признаков путем преобразования или комбинирования оригинальных признаков.
Пример: предсказание стоимости жилья.
	Признаки: $x_i$ - площадь квартиры (м. кв.), $x_2$ - город (категориальный)
	Модель: $g_1(x, \theta) = \theta_2 x_2 + \theta_1 x_1 + \theta_0$
	Добавляем новый признак $x_3 = x_2 x_1$, чтобы напрямую учесть в модели различия стоимости кв. метра в разных регионах.
	$g_2(x, \theta) = \theta_3 x_3 + \theta_2 x_2 + \theta_1 x_1 + \theta_0$ 
Способы конструирования признаков:
- Полиномиальные признаки: возведение существующих признаков в степень или их комбинирование
- Агрегация данных: среднее, сумма или медиана по имеющимся признакам
- Лаги - значение за предыдущие периоды, которые могут влиять на текущие
- Временные признаки - день недели, месяц или номер квартала, которые учитывают сезонные (периодические) изменения в данных
- Знания предметной области.