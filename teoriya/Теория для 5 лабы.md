## Embedding:
используется для преобразования категориальных данных, таких как последовательности индексов (например, закодированные слова), в плотные векторные представления заданной размерности. Этот слой чаще всего применяется в задачах обработки естественного языка (NLP), где каждое слово в предложении представляется как уникальный индекс, а слой `Embedding` преобразует эти индексы в векторы.

- **`input_dim`:**
    
    - Размер словаря (количество уникальных токенов).
    - Например, если у вас 10,000 уникальных слов в тексте, то `input_dim=10000`.
- **`output_dim`:**
    
    - Размерность векторного представления (эмбеддинга) для каждого слова.
    - Например, если `output_dim=128`, то каждое слово будет преобразовано в вектор из 128 чисел.
- **`input_length`:**
    
    - Длина входной последовательности.
    - Если указано, выходной тензор будет иметь фиксированный размер `(batch_size, input_length, output_dim)`.
- **Веса:**
    
    - Слой `Embedding` имеет матрицу весов размерности `(input_dim, output_dim)`, которая обучается вместе с моделью.
    - Эта матрица называется **матрицей эмбеддингов** и настраивается таким образом, чтобы близкие по значению слова получали похожие векторы.


##### Как работает слой `Embedding`?

- На вход слой принимает последовательность индексов (например, `[3, 7, 1, 9]`), где каждый индекс соответствует определенному токену в словаре.
- Слой берет каждый индекс и извлекает соответствующий вектор из матрицы эмбеддингов.
- На выходе получается тензор размером `(batch_size, sequence_length, output_dim)`.


##### Из методы
Слой Embedding лучше всего воспринимать как словарь, отображающий целочисленные индексы (обозначающие конкретные слова) в плотные векторы. Он принимает целые числа на входе, отыскивает их во внутреннем словаре и возвращает соответствующие векторы.
Слой Embedding получает на входе двумерный тензор с целыми числами и с формой (образцы, длина_последовательности), каждый элемент которого является последовательностью целых чисел.

- Все последовательности в пакете должны иметь одинаковую длину, потому что упаковываются в один тензор, поэтому короткие последовательности, если они есть, нужно дополнить нулями, а длинные — усечь.
- Этот слой возвращает трехмерный тензор с вещественными числами и с формой (образцы, длина_последовательности, размерность_векторного_представления).
- При создании слоя Embedding, его веса (внутренний словарь векторов токенов) инициализируются случайными значениями, как в случае с любым другим слоем. В процессе обучения векторы слов постепенно корректируются посредством обратного распространения ошибки, и пространство превращается в структурированную модель, пригодную к использованию. После полного обучения пространство векторов приобретет законченную структуру, специализированную под решение конкретной задачи.

## Векторизация

Векторизация – это процесс преобразования данных в числовые тензоры, пригодные для подачи в нейронные сети и другие алгоритмы машинного обучения. В данном контексте функция `vectorize_sequences` используется для представления текстовых данных (например, последовательностей слов или индексов) в виде бинарных векторов фиксированной размерности.

Процессы векторизации текста бывают разных видов и форм, но все протекают по одному шаблону: 
- стандартизация текста для упрощения обработки: например, все символы преобразуются в нижний регистр или из текста удаляются знаки препинания; 
- токенизация – разбиение текста на единицы (токены) — символы, слова или группы слов;
- преобразование каждого токена в числовой вектор, обычно путем индексации всех токенов, присутствующих в данных.


Токенизация методом прямого кодирования (one-hot encoding) — наиболее используемый и простой способ преобразования токенов в векторы. Он заключается в присваивании каждому слову уникального целочисленного индекса i с последующим его преобразованием в бинарный вектор размера N (размер словаря); все элементы этого вектора содержат нули, кроме i-го элемента, которому присваивается 1.

#### Параметры функции:

- **`sequences`:** Список последовательностей, где каждая последовательность — это список индексов (например, `[3, 5, 7]`).
    
- **`dimension`:** Размерность результирующего вектора. Обычно равна размеру словаря (например, 10,000 уникальных слов).



## каким образом можно оптимизировать прямое кодирование?

- Прямое хеширование признаков (one-hot hashing trick):
	- Используется, когда словарь содержит слишком большое количество токенов, чтобы его можно был использовать явно.
	- Вместо этого явного присваивания индекса каждому слову и сохранения ссылок на эти индексы в словаре можно хешировать слова в векторы фиксированного размера
	- Обычно используются очень легковесные функции
	**Главное достоинство**: отсутствие необходимости хранить индексы слов, что позволяет сэкономить память и кодировать данные по мере необходимости
	**Единственный недостаток**: возможные хеш-коллизии

- на практике чаще используется другой способ — построение индекса (словаря) всех терминов, найденных в обучающих данных, и назначение уникального целого числа каждому его элементу. Например так
![[Pasted image 20241219143114.png]]


![[Pasted image 20241219143454.png]]
- на всякий случай, может понадобится: униграмм, биграмм - кодирование мешком слов


## последовательная или не последовательная векторизация для последовательности слоев

Чтобы реализовать модель последовательности, необходимо сначала представить входные образцы в виде последовательностей целочисленных индексов (одно целое число соответствует одному слову). 

Затем каждое целое число нужно отобразить в вектор, чтобы получить последовательности векторов.

Наконец, эти последовательности векторов следует передать в стек слоев, которые смогут оценить коррелирующие признаки из соседних векторов, как это делают, например, одномерная сверточная сеть, рекуррентная нейронная сеть или модель Transformer.


Векторизация данных для последовательных и непоследовательных слоев нейронных сетей отличается в зависимости от структуры данных, целей обработки и архитектуры слоев. Рассмотрим подробнее, что такое последовательная и непоследовательная векторизация и как она применяется в нейронных сетях.

1. Последовательная векторизация используется для представления данных, которые имеют временной, порядковый или последовательный характер. Например:

- Текстовые данные (последовательность слов или токенов).
- Временные ряды.
- Последовательности данных с временными шагами.

2. Непоследовательная векторизация применяется к данным, где порядок элементов не важен, или данные представляют собой уже обработанные признаки (например, табличные данные, мешок слов, TF-IDF).

| **Характеристика**      | **Последовательная**                        | **Непоследовательная**                     |
|--------------------------|---------------------------------------------|--------------------------------------------|
| **Тип данных**          | Последовательные (текст, временные ряды).  | Табличные, статические (мешок слов, TF-IDF). |
| **Сохранение порядка**  | Да.                                        | Нет.                                       |
| **Форма данных**        | (samples, timesteps, features).            | (samples, features).                       |
| **Используемые слои**   | Embedding, LSTM, GRU, RNN.                 | Dense, Dropout, BatchNormalization.        |
| **Пример задачи**       | Машинный перевод, анализ последовательностей. | Классификация текста, регрессия.           |
